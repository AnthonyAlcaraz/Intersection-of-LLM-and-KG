Recent advances in large language models (LLMs) show impressive fluency and adaptability

But LLMs struggle with deeper reasoning requiring:
	- Compositional generalization
	- Sustained causal chains
	
Knowledge graphs provide structured representations to address these gaps

However, knowledge graphs have challenges with:
	- Scale
	- Noise
	- Incompleteness
	- Sparsity

Proposes a coordinated approach leveraging strengths of both representations

The Intersection of Graphs and LLMs
Reciprocal synergy allowing each field to enhance capabilities of the other

Graphs as Context Providers (in  context learning)
	- Append structured graph context to prompts
	- Enhances answers with factual knowledge
	- Reduces hallucination risks
	- Can reason over graph algo output, causal relationship and weighting

In context learning further enhanced by two more links :

Graphs as a Reasoning Topology (Graph of thought, Monte Carlo, Langraph, LLM compiler)
	- Graph pathways guide reasoning notably for agents
	- Model analysis as graph traversals
	- Shape overall reasoning architecture (acyclical, cyclical)


Using LLMs for Graph Tasks (
	- Adapt LLMs via pre-training and prompting
	- Frame graph problems in natural language
	- Output solutions to graph analysis tasks

![image](https://github.com/AnthonyAlcaraz/Intersection-of-LLM-and-KG/assets/127010830/7f940679-a690-47f8-a6f4-8d7fca60d9ff)
