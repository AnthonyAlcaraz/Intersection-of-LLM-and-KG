{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomasonjo/blogs/blob/master/llm/openaifunction_constructing_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cllpFMqCUaA9",
        "outputId": "7df89ff8-292c-4825-8f82-2e90e5827801"
      },
      "outputs": [],
      "source": [
        "!pip install langchain neo4j openai wikipedia tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install llamaindex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Retrieval Augmented Systems\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "username = os.getenv('NEO4J_USERNAME')\n",
        "password = os.getenv('NEO4J_PASSWORD')\n",
        "url = os.getenv('NEO4J_URI')\n",
        "database = \"neo4j\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import (\n",
        "    KnowledgeGraphIndex,\n",
        "    LLMPredictor,\n",
        "    ServiceContext,\n",
        "    SimpleDirectoryReader,\n",
        ")\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "from llama_index.graph_stores import Neo4jGraphStore\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "\n",
        "# define LLM\n",
        "llm = OpenAI(temperature=0.2, model=\"gpt-3.5-turbo-16k-0613\") #gpt-4-0613 (or) #gpt-4-1106-preview\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_store = Neo4jGraphStore(\n",
        "    username=username,\n",
        "    password=password,\n",
        "    url=url,\n",
        "    database=database,\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"\"\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    url=url,\n",
        "    username=username,\n",
        "    password=password,\n",
        "    index_name='tasks',\n",
        "    node_label=\"Task\",\n",
        "    text_node_properties=['name', 'description', 'status'],\n",
        "    embedding_node_property='embedding',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index import load_index_from_storage\n",
        "\n",
        "storage_context = StorageContext.from_defaults(persist_dir='./FinanceKG', graph_store=graph_store)\n",
        "\n",
        "kg_index = load_index_from_storage(\n",
        "    storage_context=storage_context,\n",
        "    index_id=\"807b9792-2357-4cdd-84dc-8bb39d1e7e39\",\n",
        "    max_triplets_per_chunk=8,\n",
        "    service_context=service_context,\n",
        "    include_embeddings=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = kg_index.as_query_engine(\n",
        "    include_text=True,\n",
        "    response_mode=\"tree_summarize\",\n",
        "    embedding_mode=\"hybrid\",\n",
        "    similarity_top_k=12,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(streaming=True)\n",
        "streaming_response = query_engine.query(\"How can you analyze assets\")\n",
        "streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.memory import ChatMemoryBuffer\n",
        "\n",
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1000)\n",
        "\n",
        "chat_engine = kg_index.as_chat_engine(\n",
        "    chat_mode=\"openai\",\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    system_prompt=\"You are an AI chatbot with a specific focus on the Reactive Insight application and corporate finance. Your knowledge is strictly derived from a comprehensive Knowledge Graph. You are not permitted to provide any information that is not contained within the Knowledge Graph. Ensure all responses are relevant to these topics, detailed, and accurate. Remember, you are here to facilitate understanding, so be as pedagogical as possible. Also, propose any other relevant questions that can be deduced from the user's query. If a query cannot be answered with the information in the Knowledge Graph, politely inform the user that the information is not available.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = chat_engine.chat(\"What is the Super priorization algorithm? \", function_call=\"query_engine_tool\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.indices.knowledge_graph import KnowledgeGraphRAGRetriever\n",
        "from llama_index.agent import OpenAIAgent\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.memory import ChatMemoryBuffer\n",
        "\n",
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1000)\n",
        "\n",
        "# Your knowledge graphs\n",
        "my_kgs = {'kg1': kg_index, 'kg2': kg_index2}\n",
        "\n",
        "# Dictionary to store the agents\n",
        "kg_agents = {}\n",
        "\n",
        "# List to store the tools\n",
        "kg_tools = []\n",
        "\n",
        "for kg_name, kg in my_kgs.items():\n",
        "    # Create a query engine for the KG\n",
        "    query_engine = kg.as_query_engine(\n",
        "        include_text=True,\n",
        "        response_mode=\"tree_summarize\",\n",
        "        embedding_mode=\"hybrid\",\n",
        "        similarity_top_k=20,\n",
        "    )\n",
        "    \n",
        "    # Create a tool for the query engine\n",
        "    tool = QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=f\"tool_{kg_name}\",\n",
        "            description=f\"Useful for questions related to {kg_name}\",\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    # Add the tool to the list of KG tools\n",
        "    kg_tools.append(tool)\n",
        "    \n",
        "    # Create an agent for the tool\n",
        "    agent = OpenAIAgent.from_tools([tool], system_prompt=\"Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.\")\n",
        "    \n",
        "    # Add the agent to the dictionary of KG agents\n",
        "    kg_agents[kg_name] = agent\n",
        "\n",
        "# Create the super agent\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "super_agent = OpenAIAgent.from_tools(kg_tools, llm=llm, verbose=True, memory=memory, system_prompt=\"Therefore, the answer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.indices.knowledge_graph import KnowledgeGraphRAGRetriever\n",
        "from llama_index.agent import OpenAIAgent\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.memory import ChatMemoryBuffer\n",
        "\n",
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1000)\n",
        "\n",
        "# Your knowledge graphs\n",
        "my_kgs = {'kg1': kg_index, 'kg2': kg_index2}\n",
        "\n",
        "# Dictionary to store the agents\n",
        "kg_agents = {}\n",
        "\n",
        "# List to store the tools\n",
        "kg_tools = []\n",
        "\n",
        "for kg_name, kg in my_kgs.items():\n",
        "    # Create a query engine for the KG\n",
        "    query_engine = kg.as_query_engine(\n",
        "        include_text=True,\n",
        "        response_mode=\"tree_summarize\",\n",
        "        embedding_mode=\"hybrid\",\n",
        "        similarity_top_k=20,\n",
        "    )\n",
        "    \n",
        "    # Create a tool for the query engine\n",
        "    tool = QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=f\"tool_{kg_name}\",\n",
        "            description=f\"Useful for questions related to {kg_name}\",\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    # Add the tool to the list of KG tools\n",
        "    kg_tools.append(tool)\n",
        "    \n",
        "    # Create an agent for the tool\n",
        "    agent = OpenAIAgent.from_tools([tool], system_prompt=\"Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.\")\n",
        "    \n",
        "    # Add the agent to the dictionary of KG agents\n",
        "    kg_agents[kg_name] = agent\n",
        "\n",
        "# Create the super agent\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "super_agent = OpenAIAgent.from_tools(kg_tools, llm=llm, verbose=True, memory=memory, system_prompt=\"\"\"Query: [Question]?\n",
        "Knowledge Source: [Documents/data used to answer question]  \n",
        "\n",
        "Structured Knowledge Evidence:\n",
        "- (Subject1, Relation1, Object1) \n",
        "- (Subject2, Relation2, Object2)\n",
        "[At least 2 factual knowledge triples to ground reasoning]\n",
        "\n",
        "Interpretation:  \n",
        "[1-2 sentence natural language interpretation of reasoning process using provided knowledge]\n",
        "\n",
        "Reliability Assessment:\n",
        "- Factuality Score: [Numeric score 0-1 rating factuality of knowledge evidence]  \n",
        "- Faithfulness Score: [Numeric score 0-1 rating relation of interpretation to overall answer]\n",
        "\n",
        "[If scores are low:]\n",
        "Corrected Knowledge:\n",
        "- (Corrected Subject, Relation, Object)\n",
        "[Injected corrected factual knowledge from external source]\n",
        "\n",
        "Re-interpretion:  \n",
        "[Updated natural language interpretation if needed based on corrected knowledge] \n",
        "\n",
        "Conclusion:\n",
        "[Final answer to original question]\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = await super_agent.astream_chat(\"Analyze what to do to recuce costs ?\")\n",
        "\n",
        "# Collect all tokens into a string\n",
        "response_text = \"\"\n",
        "async for token in response.async_response_gen():\n",
        "    response_text += token\n",
        "\n",
        "# Print the response text\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initial question\n",
        "initial_question = \"Define assets? And optimize them\"\n",
        "\n",
        "# Query the ontology with the initial question\n",
        "ontology_results = ontology_engine.query(initial_question)\n",
        "\n",
        "# Combine the initial question with the ontology results\n",
        "combined_question = f\"{initial_question}. {ontology_results}\"\n",
        "\n",
        "# Query the super agent with the combined question\n",
        "response = await super_agent.astream_chat(combined_question)\n",
        "\n",
        "# Collect all tokens into a string\n",
        "response_text = \"\"\n",
        "async for token in response.async_response_gen():\n",
        "    response_text += token\n",
        "\n",
        "# Print the response text\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "response = vector_index.similarity_search(\n",
        "    \"How will RecommendationService be updated?\"\n",
        ")\n",
        "print(response[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "vector_qa = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(), chain_type=\"stuff\", retriever=vector_index.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_qa.run(\n",
        "    \"How many open tickets there are?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "graph.query(\n",
        "    \"MATCH (t:Task {status:'open'}) RETURN count(*)\"\n",
        ")\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import GraphCypherQAChain\n",
        "\n",
        "graph.refresh_schema()\n",
        "\n",
        "cypher_chain = GraphCypherQAChain.from_llm(\n",
        "    cypher_llm = ChatOpenAI(temperature=0, model_name='gpt-4'),\n",
        "    qa_llm = ChatOpenAI(temperature=0), graph=graph, verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Tasks\",\n",
        "        func=vector_qa.run,\n",
        "        description=\"\"\"Useful when you need to answer questions about descriptions of tasks.\n",
        "        Not useful for counting the number of tasks.\n",
        "        Use full question as input.\n",
        "        \"\"\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Graph\",\n",
        "        func=cypher_chain.run,\n",
        "        description=\"\"\"Useful when you need to answer questions about microservices,\n",
        "        their dependencies or assigned people. Also useful for any sort of\n",
        "        aggregation like counting the number of tasks, etc.\n",
        "        Use full question as input.\n",
        "        \"\"\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "mrkl = initialize_agent(\n",
        "    tools, ChatOpenAI(temperature=0, model_name='gpt-4'), agent=AgentType.OPENAI_FUNCTIONS, verbose=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOtPgbncRs178mcTTl511e3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
